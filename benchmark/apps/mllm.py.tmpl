"""An app that runs a Multimodal LLM task."""

from __future__ import annotations

from collections.abc import AsyncIterator

from cornserve.app.base import AppConfig
from cornserve_tasklib.task.unit.encoder import Modality
from cornserve_tasklib.task.composite.llm import $TASK_CLASS
from cornserve_tasklib.task.unit.llm import OpenAIChatCompletionChunk, OpenAIChatCompletionRequest

mllm = $TASK_CLASS(
    model_id="$MODEL_ID",
    modalities=[Modality.IMAGE],
    encoder_fission=$ENCODER_FISSION,
)

class Config(AppConfig):
    """App configuration model."""

    tasks = {"mllm": mllm}


async def serve(request: OpenAIChatCompletionRequest) -> AsyncIterator[OpenAIChatCompletionChunk]:
    """Main serve function for the app."""
    return await mllm(request)
