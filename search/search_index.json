{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Cornserve","text":"Cornserve: Easy, Fast, and Scalable Multimodal AI      vLLM : Cornserve = Monolith : Microservice    <p>Multimodal AI models like Qwen 3 Omni are becoming increasingly complex and heterogeneous.</p> <p>Cornserve is a distributed serving system for multimodal AI. Cornserve performs model fission and automatic sharing of common components (e.g., LLMs, Vision Encoders, Audio Generators) across applications on your infrastructure.</p> <ol> <li>Independent scaling: Each component of complex multimodal models (e.g., LLMs, vision encoders, audio generators) can be scaled independently based on incoming request load.</li> <li>Less interference: For instance, some Vision-Language Model requests may have three images, while some may have none. When all is crammed into a single monolithic server, multimodal embedding and LLM text generation can interfere with and delay each other. Model fission allows each component to run in isolation, reducing interference and improving latency.</li> <li>Lower complexity: A single monolithic server that handles multimodal inputs, LLM text generation, and multimodal outputs is extremely complex to build and maintain. Cornserve is the substrate that allows the composition of simpler task executors (microservices) into complex multimodal AI applications.</li> </ol> <ul> <li> <p> Model fission</p> <p>Split up your complex models into smaller components and scale them independently.</p> </li> <li> <p> Automatic sharing</p> <p>Common model components are automatically shared across applications.</p> </li> <li> <p> Multimodal-native</p> <p>Cornserve is built multimodal-native from the ground up. Image, video, audio, and text are all first-class citizens.</p> </li> <li> <p> Simple K8s deployment</p> <p>One-command deployment to Kubernetes with Kustomize.</p> </li> <li> <p> Observability</p> <p>Built-in support for OpenTelemetry to monitor your apps and requests.</p> </li> <li> <p> Open Source, Apache-2.0</p> <p>Cornserve is open-source with the Apache 2.0 license and is available on GitHub.</p> </li> </ul>"},{"location":"architecture/","title":"Cornserve Architecture","text":""},{"location":"architecture/#cornserve-architecture","title":"Cornserve Architecture","text":"<p>Cornserve is a distributed multimodal AI application serving platform that allows you to implement and deploy ML applications on your infrastructure.</p>"},{"location":"architecture/#task-and-app","title":"Task and App","text":"<p>Applications are written by developers using <code>cornserve.app.base</code>. It must define two things:</p> <ul> <li>A config class that inherits from <code>cornserve.app.base.AppConfig</code>, whose main purpose is to specify the tasks that the app intends to invoke (more on tasks soon).</li> <li><code>async def serve(request: RequestT) -&gt; ResponseT</code>: The main function that handles the request and returns a response. <code>RequestT</code> must be a subclass of <code>pydantic.BaseModel</code> and <code>ResponseT</code> should either be a subclass of <code>pydantic.BaseModel</code> (non-streaming response) or an <code>AsyncIterator</code> that yields a subclass of <code>pydantic.BaseModel</code> (streaming response).</li> </ul> <p>This is a quick example app that provides multimodal LLM inference, and it peeks into the generated tokens to look whether someone mentioned \"Cornserve\":</p> <pre><code>from collections.abc import AsyncIterator\n\nfrom cornserve_tasklib.task.composite.llm import MLLMTask\nfrom cornserve_tasklib.task.unit.encoder import Modality\nfrom cornserve_tasklib.task.unit.llm import (\n    OpenAIChatCompletionChunk,\n    OpenAIChatCompletionRequest,\n)\n\nfrom cornserve.app.base import AppConfig\n\nmllm = MLLMTask(\n    model_id=\"google/gemma-3-4b-it\",\n    modalities=[Modality.IMAGE],\n    encoder_fission=True,\n)\n\n\nclass Config(AppConfig):\n    \"\"\"App configuration model.\"\"\"\n\n    tasks = {\"mllm\": mllm}\n\n\nasync def serve(\n    request: OpenAIChatCompletionRequest,\n) -&gt; AsyncIterator[OpenAIChatCompletionChunk]:\n    \"\"\"Main serve function for the app.\"\"\"\n    async for chunk in await mllm(request):\n        token = chunk.choices[0].delta.content\n        if token == \"Cornserve\":\n            print(\"Yay found Cornserve!\")\n        yield chunk\n</code></pre> <p>Importantly, in app configurations, apps specify the tasks that they intend to invoke. These tasks are dispatched to be executed on the data plane. Tasks are imported from modules under <code>cornserve_tasklib.task</code>, such as <code>MLLMTask</code> for multimodal LLM inference, <code>LLMTask</code> for LLM inference, and <code>EncoderTask</code> for multimodal data embedding, and users can build their own tasks using components from <code>cornserve.task.base</code>. All other inline Python code is executed imperatively by the Cornserve Gateway, so the app offers the full flexibility of Python programming.</p> <p>Another part to highlight is <code>encoder_fission=True</code> passed into <code>MLLMTask</code>. When it's <code>True</code>, the image encoder and the LLM model will be split and deployed as two separate Task Executors (i.e., two separate unit tasks) on the data plane. Each Task Executor in this case will run on dedicated GPUs. On the other hand, if <code>encoder_fission=False</code>, both the image encoder and the LLM model will be deployed together as a single Task Executor on the data plane, sharing the same GPUs -- this is what monolithic LLM serving systems today do. So you can always pick and choose whether you want to use encoder fission or not on a per-app basis based on the characteristics of your workload.</p> <p>For a more realistic example, see Building Apps. See also the dedicated page on tasks.</p>"},{"location":"architecture/#control-plane","title":"Control Plane","text":"<p>The control plane manages numerous registered apps and handles incoming requests to each app.</p> <p>Control plane components generally send and receive control signals using gRPC (<code>proto/v1/*.proto</code>). On the other hand, application requests and task invocations are sent and received using HTTP.</p>"},{"location":"architecture/#gateway-and-app-manager","title":"Gateway and App Manager","text":"<p>Package: <code>cornserve.services.gateway</code></p> <p>The gateway is the entry point for all apps and incoming requests to each app.</p> <p>An app is registered with Cornserve by the cluster admin by sending a request to the gateway, including the app's Python source code as string. The gateway then validates the app definition (primarily whether it has all the required classes and the <code>serve</code> function) and registers it with the App Manager singleton class.</p> <p>When a new app is registered, the App Manager will read in the tasks that the app intends to invoke and instruct the Resource Manager to deploy Task Managers in the data plane such that all tasks invoked by the new app is available for execution in the data plane. There is only a single Task Manager per task, so multiple apps that invoke the same task will share a single Task Manager.</p> <p>When a request for a registered app is received, the gateway will spawn a new App Driver for the app to handle the request. The App Driver will execute the app's <code>serve</code> function, and invoked tasks will be sent to the Task Dispatcher, which handles actually executing the task in the data plane and retrieving results back to the App Driver.</p>"},{"location":"architecture/#resource-manager","title":"Resource Manager","text":"<p>Package: <code>cornserve.services.resource_manager</code></p> <p>The Resource Manager is primarily responsible for allocating cluster resources (primarily GPUs) to Task Managers.</p> <p>There are two primary events that trigger the Resource Manager to allocate resources:</p> <ol> <li>New app registration. When a new app is registered and its required tasks are sent to the Resource Manager, the Resource Manager figures out the Task Managers that need to be deployed in the data plane. If there was already an app that required some of the tasks, those Task Managers will not be deployed again, but rather shared by those apps.</li> <li>App unregistration. When an app is unregistered, the Resource Manager will check if there are any other apps that require the same tasks. If not, those unnecessary Task Managers will be killed.</li> </ol> <p>Beyond app registration and unregistration, the Resource Manager also dynamically adjusts the amount of resources given to each Task Manager. Say, if a certain Task Manager receives more requests than others, or if it is computationally heavy and cannot serve as many requests per second compared to other tasks, the Resource Manager will dynamically provision more resources for it. This will happen at the cost of taking away resources from other Task Managers, if need be. The goal would be to balance the request throughput of the whole system over time given a fixed amount of resource.</p>"},{"location":"architecture/#task-manager","title":"Task Manager","text":"<p>Package: <code>cornserve.services.task_manager</code></p> <p>A Task Manager is responsible for executing a single unit task given a subset of the cluster's resources and exposing information about their performance characteristics. A task, for instance, can be an LLM inference task with a particular model; an LLM inference task with a different model, for instance, is considered a different task.</p> <p>Task Managers spawn one or more Task Executors that will actually perform task execution on GPUs on the data plane. The Task Manager is responsible for managing the lifecycle of the Task Executors, including spawning and killing them as needed. When there are more than one Task Executors deployed under a Task Manager, the Task Manager will also route and load balance requests across the Task Executors.</p> <p>For multimodal data embedding tasks, the Task Manager will use Eric as the Task Executor by default. For multimodal content generation tasks, the Task Manager will use Geri as the Task Executor by default. Finally, for LLM inference tasks, the Task Manager will use our fork of vLLM as the Task Executor.</p> <p>The Task Manager also exposes performance characteristics of the Task Executors. For instance, given \\(N\\) GPUs, the Task Manager will profile the Task Executor's throughput and latency and expose the throughput--latency Pareto frontier. The Resource Manager can make better resource allocation decisions based on this information.</p>"},{"location":"architecture/#task-dispatcher","title":"Task Dispatcher","text":"<p>Package: <code>cornserve.services.task_dispatcher</code></p> <p>App Drivers or interactive Jupyter notebook users send task invocation requests to the Task Dispatcher, which is responsible for dispatching the requests to appropriate Task Executors and retrieving the results back to the App Driver.</p> <p>When a composite task is invoked, the following happens:</p> <ol> <li>The composite task's <code>__call__</code> method records the unit task invocations and dispatches all of them to the Task Dispatcher.<ul> <li>For instance, if the composite task is a Vision-Language Model task, its <code>__call__</code> method will record two unit task invocations: one for the image encoder and one for the LLM text generation.</li> </ul> </li> <li>(For each unit task invocation) The Task Dispatcher queries the Task Manager for the Task Executor that is best suited to handle the request.</li> <li>(For each unit task invocation) The Task Dispatcher translates the <code>TaskInput</code> object of the unit task invocation into JSON, dispatches the HTTP request to the selected Task Executor, waits for the result, and translates the result back to a <code>TaskOutput</code> object.</li> <li>Finally, the Task Dispatcher aggregates all unit task invocation results and response with the final result.</li> </ol> <p>How does the Task Dispatcher know how to contact the Task Manager? Whenever there is a change to Task Managers (spawning new ones or killing existing ones), the Resource Manager will inform the Task Dispatcher of the mapping between the unit task definition and its corresponding Task Manager's endpoint.</p> <p>The Task Dispatcher is horizontally replicated (currently default to 3 replicas) to prevent it from being a bottleneck.</p>"},{"location":"architecture/#data-plane","title":"Data Plane","text":"<p>The data plane is where the actual task execution happens on GPUs.</p>"},{"location":"architecture/#task-executor","title":"Task Executor","text":"<p>Package: <code>cornserve.task_executors</code></p> <p>As detailed in the Task page, a single unit task class is associated with a Task execution descriptor, which provides information about how to spin up the Task Executor and how to execute the task, among other things.</p> <p>Refer to Eric and Geri and vLLM for more information about built-in Task Executors. Cornserve currently uses our fork of vLLM v0.11.1.</p>"},{"location":"architecture/#sidecar","title":"Sidecar","text":"<p>Package: <code>cornserve.services.sidecar</code></p> <p>Data plane Task Executors need to communicate tensor data between each other. A concrete example would be Eric sending the encoded image/video tensor to vLLM for text generation. See Sidecar for more about the Sidecar service.</p>"},{"location":"architecture/eric_and_geri/","title":"Eric and Geri","text":""},{"location":"architecture/eric_and_geri/#eric-and-geri-multimodal-task-executors","title":"Eric and Geri: Multimodal Task Executors","text":"<p>Mosharaf: Hey what is this \"Eric\" thing in the architecture diagram? Jae-Won: Oh. Uh, no. It says \"Enc.\" For Encoder. Mosharaf: Oh. Jae-Won: Now it's Eric.</p>"},{"location":"architecture/eric_and_geri/#eric-multimodal-data-embedding-server","title":"Eric: Multimodal Data Embedding Server","text":"<p>Package: <code>cornserve.task_executors.eric</code></p> <p>Eric is a multimodal data embedding server that takes in a list of multimodal data (e.g., images, videos) and computes the multimodal embedding of the input data.</p>"},{"location":"architecture/eric_and_geri/#architecture","title":"Architecture","text":"<p>Below, components are divided at the process boundary.</p>"},{"location":"architecture/eric_and_geri/#router","title":"Router","text":"<p>The gateway router is an async FastAPI server that (1) receives modality encoding requests and (2) preprocesses modality data before running the encoder. Preprocessing is done asynchronously in a thread pool by the <code>eric.router.processor.Processor</code> class.</p> <p>Each model processes different modality data differently, so the router must instantiate the correct model-specific preprocessor. Instantiating and invoking these model- and modality-specific preprocessors are implemented in the class <code>eric.models.[model_module].ModalityProcessor</code>, which is a subclass of <code>eric.models.base.BaseModalityProcessor</code>.</p> <p>When modality preprocessing is complete, the router submits the embedding request to the engine. The router and the engine communicate through ZMQ sockets. Especially, the router holds an instance of the engine client (<code>eric.engine.client.EngineClient</code>), which is used to send requests to the engine and receive responses.</p>"},{"location":"architecture/eric_and_geri/#engine","title":"Engine","text":"<p>From the engine and below, everything is synchronous Python (i.e., not <code>asyncio</code>).</p> <p>The Engine constantly receives embedding requests from the router, runs the request scheduler to create a <code>eric.schema.Batch</code>, and invokes the model executor (<code>eric.executor.executor.ModelExecutor</code>) to compute the multimodal embedding. The model executor provides the <code>execute_model</code> method, which broadcasts input batch data to all Workers via shared memory.</p> <p>The engine currently only batches data of the same modality together. This is because there are models that have different code paths for different modalities. Furthermore, due to the compute-intensive nature of multimodal encoders, it is unlikely we will scale to large batch sizes.</p>"},{"location":"architecture/eric_and_geri/#workers","title":"Workers","text":"<p>There is one worker (<code>eric.executor.worker.Worker</code>) process per GPU. The number of workers is the tensor parallelism degree. When spawned, the workers initialize PyTorch distributed and instantiate the model from weights downloaded from the Hugging Face Hub. It then waits for the model executor to dispatch a batch to it, runs tensor parallel inference, and dispatches tensor communication to the destination Task Executor via the sidecar.</p>"},{"location":"architecture/eric_and_geri/#geri-multimodal-content-generation-server","title":"Geri: Multimodal Content Generation Server","text":"<p>Package: <code>cornserve.task_executors.geri</code></p> <p>Geri (pronounced \"Jerry\") is a multimodal content generation server that takes in embeddings and generates multimodal content such as images, videos, or audio. It is the counterpart to Eric\u2014where Eric encodes multimodal data into embeddings, Geri decodes embeddings into generated content.</p>"},{"location":"architecture/eric_and_geri/#architecture_1","title":"Architecture","text":"<p>Geri shares the same architectural design as Eric, with components divided at the process boundary:</p> <p>Router: An async FastAPI server (<code>geri.router</code>) that receives generation requests via the <code>/generate</code> endpoint. The router communicates with the engine through ZMQ sockets using an <code>EngineClient</code> (<code>geri.engine.client.EngineClient</code>), similar to Eric's design.</p> <p>Engine: A synchronous Python engine (<code>geri.engine.core</code>) that receives generation requests from the router, runs the request scheduler to create batches, and invokes the model executor (<code>geri.executor.executor.ModelExecutor</code>) to generate the requested content. Like Eric, the engine batches requests and broadcasts input data to workers via shared memory.</p> <p>Workers: One worker process per GPU (<code>geri.executor.worker.Worker</code>), matching the tensor parallelism degree. Workers initialize PyTorch distributed, load the generative model from the Hugging Face Hub, and perform tensor parallel inference to generate content. Generated outputs are communicated via the sidecar when needed.</p> <p>The key difference from Eric is in the task being performed: Geri performs generative inference (embeddings to content) rather than encoding (content to embeddings), but the overall system architecture and communication patterns remain the same.</p>"},{"location":"architecture/sidecar/","title":"Sidecar","text":""},{"location":"architecture/sidecar/#sidecar-p2p-communication-service","title":"Sidecar: P2P Communication Service","text":"<p>Package: <code>cornserve.services.sidecar</code> (server) and <code>cornserve.sidecar.api</code> (client)</p> <p>Sidecar is a P2P communication service in Cornserve that allows task executors to send/receive intermediate data to/from each other. It's mainly designed for tensors, but it also supports byte-serializable Python objects like strings.</p> <p>Sidecars are intended to be integrated into any Task Executor that needs to communicate with other Task Executors.</p>"},{"location":"architecture/sidecar/#rationale","title":"Rationale","text":"<p>One obvious way to do P2P communication of tensors across the cluster is to use NCCL. However, there are several problems with this approach:</p> <ol> <li>NCCL has a fixed world size that cannot be changed. Faults in one rank will disrupt the whole cluster.</li> <li>NCCL is bound to the process that creates the communicator. This means that NCCL is not amenable to Cornserve auto-scaling Task Manager resources which lead to Task Executors being killed and spawned.</li> <li>NCCL spawns a high-speed polling CUDA kernel that takes up the GPU's SM, potentially leading to performance degradation for actual computation tasks that should be running on the GPU.</li> <li>NCCL may also use NVLink for tensor transfer, which can interfere with the NVLink bandwidth needs for Task Executors (e.g., tensor parallelism).</li> </ol> <p>Instead, each Task Executor runs alongside a long-running Sidecar server, and performs P2P communication with other Task Executors via the sidecar servers. This liberates the Task Executors from the constraints of NCCL.</p>"},{"location":"architecture/sidecar/#architecture","title":"Architecture","text":"<p>Sidecars have servers and clients. Servers are long running services in the cluster, created upon cluster deployment. Clients live within Task Executors, and task executors invoke clients for send and receive operations which are then fulfilled by the servers.</p>"},{"location":"architecture/sidecar/#servers","title":"Servers","text":"<p>Each GPU in the cluster is usually paired with one dedicated Sidecar server, but Sidecar servers can also act as a group when, for example, a task executor runs a model with tensor parallel.</p> <p>All Sidecar servers and clients send control signals through gRPC, while Sidecar servers use <code>UCX</code> as the backend for tensor transfer, which uses RDMA if possible. Small objects are directly sent over through gRPC to reduce contention.</p>"},{"location":"architecture/sidecar/#forwarding-tensors","title":"Forwarding Tensors","text":"<p>Tensor transfer among Sidecar servers does not use NVLink to reserve it for distributed inference like tensor parallelism. Throughout a tensor forward from a producer with a Sidecar sender server to a consumer with a Sidecar receiver server, the Sidecar sender will copy the tensor from the producer's GPU to CPU, and the tensor will arrive at the receiver server's CPU. Therefore, the consumer has the responsibility for copying the received tensor to its devices. If the producer and the consumer locates within the same node, there will be no addition transfer over the network.</p> <p>When multiple Sidecars are grouped, Sidecars assume each producer in the group holds a full replica of the tensor to forward, and the Sidecar Servers could choose to either use one single GPU or use every GPU in the group when copying -- adjusted through a configuration knob.</p>"},{"location":"architecture/sidecar/#chunking","title":"Chunking","text":"<p>Producers are free to chunk the forwarding tensors in any way. However, it's not recommended to have chunks with non-contiguous memory view due overhead. Sidecars view each chunk as independent, so there is no guarantee that all the chunks will be in order or are placed in a slab of contiguous memory. Consumers can decide to process chunks in order, or decide to process all chunks together if the consumer cannot utilize chunks independently.</p>"},{"location":"architecture/sidecar/#memory-management","title":"Memory Management","text":"<p>Sidecar servers manage CPU memory for placing the tensors to send and receive. To reduce internal fragmentation, sidecar clients, thus task executors, are currently required to provide memory hint for the servers. The memory hint is conceptually the memory allocation unit size for the servers, and typically this could be the hidden size of a model the executor is running.</p>"},{"location":"architecture/sidecar/#clients","title":"Clients","text":"<p>The frontend API for Task Executors to interact with servers.</p> <p>Task executors can instantiate a <code>SidecarConfig</code> from <code>cornserve.sidecar.schema</code> and then instantiate a <code>Sidecar</code> client from <code>cornserve.sidecar.api</code>. The client will setup the Sidecar server for the task executor's use upon creation. The client mainly provides three sets of APIs, namely, <code>send</code>, <code>recv</code>, and <code>mark_done</code>.</p>"},{"location":"architecture/sidecar/#send","title":"<code>send</code>","text":"<p><code>send</code> can be used to broadcast some data to a list of Sidecar groups. When chunking is involved, the producer need to fill in the <code>chunk_id</code> and <code>num_chunks</code> parameters.</p>"},{"location":"architecture/sidecar/#recv","title":"<code>recv</code>","text":"<p><code>recv</code> can be used to receive data at chunk-granularity, where <code>chunk_id</code> can be specified. The returning data is either a tensor with CPU storage or a small python object. Receive operations are idempotent for Sidecars, so multiple consumer processes can consume the data concurrently. There is also a synchronous version called <code>recv_sync</code>.</p>"},{"location":"architecture/sidecar/#mark_done","title":"<code>mark_done</code>","text":"<p><code>mark_done</code> is used to free the backing memory of a received tensor in the Sidecar server. As the Sidecar server allows for idempotent receive operations, the data is always held within the server until a corresponding <code>mark_done</code> called.</p>"},{"location":"architecture/sidecar/#comparison-with-nixl","title":"Comparison with NIXL","text":"<p>It was a beautiful coincidence that the Cornserve Sidecar was designed and built at around the same time as NVIDIA's NIXL, which also provides P2P tensor communication for various purposes like disaggregated LLM inference or RL weight transfer. However, there are several differences between the two:</p> <ul> <li>Memory management: Sidecar is a long-running process that manages P2P communication between GPUs in the cluster and manages memory for tensors. On the other hand, NIXL is closer to a communication library and memory management (e.g., passing pre-allocated buffers) is left to the user.</li> <li>Communication medium: Sidecar uses shared memory (<code>/dev/shm</code>) for intra-node communication. This is to avoid any interference with NVLink/NVSwitch bandwidth that is critical for distributed inference (i.e., tensor parallelism).</li> </ul> <p>Overall, the Sidecar intends to take more responsibility for memory and communication management, and we intend to tailor it specifically for Cornserve use cases.</p>"},{"location":"architecture/task/","title":"Task","text":""},{"location":"architecture/task/#task-abstraction","title":"Task Abstraction","text":"<p>This page explains the definition of tasks as a unit of work and how the system executes them.</p>"},{"location":"architecture/task/#task","title":"<code>Task</code>","text":"<p>A <code>Task</code> is a reusable and composable unit of work that is executed within the data plane. Applications define and invoke one or more Tasks to achieve their goals. A <code>Task</code> logically describes what should be done.</p> <p>A task can be either a Unit Task, or a Composite Task. The former is a single atomic unit of work, while the latter is a composition of multiple Unit Tasks.</p>"},{"location":"architecture/task/#examples","title":"Examples","text":"<p>A <code>Task</code> can be:  </p> <ul> <li>a single inference of a neural network on GPUs (e.g., Text encoder, Vision encoder, Audio encoder, LLM, DiT, VAE decoder, Vocoder)</li> <li>a composition of the above inference units (e.g., a Vision-Language model, a Thinker-Talker architecture model)</li> </ul> <p>The former is called a Unit Task, and concrete unit tasks inherit from <code>cornserve.task.base.UnitTask</code>. On the other hand, the latter is called a Composite Task, and they simply inherit from <code>cornserve.task.base.Task</code> and declare their subtasks and their execution logic in the <code>invoke</code> method.</p> <p>Concretely, <code>MLLMTask</code> is a composition of <code>EncoderTask</code> and <code>LLMTask</code>.</p>"},{"location":"architecture/task/#properties","title":"Properties","text":""},{"location":"architecture/task/#recursive-composition","title":"Recursive Composition","text":"<p>Tasks are recursively composed; a Task can be a single inference of a neural network on GPUs or a DAG of other Tasks that make up a larger chunk of coherent work.</p>"},{"location":"architecture/task/#data-forwarding","title":"Data Forwarding","text":"<p>The inputs and outputs to a Task are defined by the Task itself, and both are stored in the App Driver. However, intermediate data (particularly tensors) are forwarded to next Tasks within the data plane via the Sidecar. That is, when a Task is composed of multiple sub-Tasks, the output of one sub-Task is forwarded to the next sub-Task in the DAG.</p>"},{"location":"architecture/task/#static-graph-given-task-input","title":"Static Graph Given Task Input","text":"<p>The concrete execution DAG of a Task must be statically determined at the time of invocation by a request. That is, the DAG must be completely determined by the App Driver given the request to the app. In other words, there can not be any dynamic control flow that depends on unmaterialized intermediate data. For instance, the input to a Task may hold a field <code>image_url: str | None</code>, and whether the execution DAG includes the image encoder can be determined by inspecting whether the <code>image_url</code> field is <code>None</code> or not.</p>"},{"location":"architecture/task/#specification","title":"Specification","text":"<p>The core specification of a Task is by its execution DAG. Each node is a Task instance that has:</p> <ul> <li>Task execution descriptor: Descriptor instance that describes how the Task is executed.</li> <li>The <code>invoke</code> method: The method that executes the Task. Input and output are Pydantic models. The Python code in <code>invoke</code> puts together the Tasks invocation to implicitly define the execution DAG.</li> </ul>"},{"location":"architecture/task/#taskexecutiondescriptor","title":"<code>TaskExecutionDescriptor</code>","text":"<p>A <code>TaskExecutionDescriptor</code> strategy class that describes how a Task is executed. Each concrete <code>Task</code> subclass is associated with one <code>TaskExecutionDescriptor</code> subclasses and takes an instance of the descriptor as an argument to its constructor.</p>"},{"location":"architecture/task/#examples_1","title":"Examples","text":"<p>The <code>LLMTask</code> is compatible with the <code>VLLMDescriptor</code>, which describes how to execute the LLM task using vLLM. Currently, only vLLM is implemented, but other executors like TensorRT-LLM or Dynamo can be implemented in the future. Similarly, the <code>EncoderTask</code> is compatible with the <code>EricDescriptor</code>, which describes how to execute the encoder task using Eric, and the <code>GeneratorTask</code> is compatible with the <code>GeriDescriptor</code>, which describes how to execute the generator task using Geri.</p>"},{"location":"architecture/task/#crd-based-task-management","title":"CRD-Based Task Management","text":"<p>Cornserve uses Kubernetes Custom Resource Definitions (CRDs) to enable dynamic task and execution descriptor management, moving away from statically built-in tasks to a more flexible runtime system.</p>"},{"location":"architecture/task/#motivation","title":"Motivation","text":"<p>Rather than having tasks and execution descriptors statically compiled into the system, the CRD-based approach allows:</p> <ul> <li>Dynamic registration: Add new task types and execution descriptors at runtime without redeploying services</li> <li>Runtime flexibility: Deploy, update, and remove tasks during system operation</li> <li>Fault tolerance: Services can recover task definitions and instances after restarts by reading from CRDs</li> <li>Single source of truth: CRDs serve as the authoritative state for task definitions, descriptors, and instances</li> </ul>"},{"location":"architecture/task/#crd-types","title":"CRD Types","text":"<p>The system defines Custom Resources for:</p> <ul> <li>Unit Task Classes: Definitions of atomic task types (e.g., <code>LLMTask</code>, <code>EncoderTask</code>)</li> <li>Composite Task Classes: Definitions of task compositions (e.g., <code>MLLMTask</code>)</li> <li>Task Execution Descriptors: Execution strategies for unit tasks (e.g., <code>VLLMDescriptor</code>, <code>EricDescriptor</code>)</li> <li>Unit Task Instances: Specific instantiations of tasks with concrete parameters</li> </ul>"},{"location":"architecture/task/#how-it-works","title":"How It Works","text":"<p>Control plane services (Gateway, Resource Manager, Task Manager, Task Dispatcher) use a <code>TaskRegistry</code> that:</p> <ol> <li>Watches CRDs: Monitors Kubernetes for task and descriptor Custom Resources using the \"list then watch\" pattern</li> <li>Loads into runtime: Dynamically loads task classes and descriptors into Python's module system when CRs are created or updated</li> <li>Maintains registries: Keeps in-memory mappings from task/descriptor names to their runtime class definitions</li> <li>References by name: Services pass task instance references (CR names) rather than serializing full task objects</li> </ol> <p>This architecture enables eventual consistency across services while leveraging Kubernetes' strong consistency guarantees for the underlying CR storage.</p> <p>Note</p> <p>The CRD-based management system is under active development and subject to change, particularly around versioning strategies and tear-down semantics.</p>"},{"location":"architecture/task/#task-lifecycle","title":"Task Lifecycle","text":""},{"location":"architecture/task/#registration","title":"Registration","text":"<p>Unit Tasks classes (e.g., <code>LLMTask</code>) are registered with the whole system. Their source code (concrete class definition) should be available to all services in the system. At the moment, we create multiple built-in Unit Task classes under <code>cornserve_tasklib.task.unit</code> and compose them under <code>cornserve_tasklib.task.composite</code>.</p>"},{"location":"architecture/task/#deployment","title":"Deployment","text":"<p>A Unit Task class that is registered in the system can be deployed on the data plane as a Unit Task instance (e.g., <code>LLMTask(model_id=\"llama\")</code>).</p> <ol> <li>The Unit Task object is instantiated externally, and then serialized into JSON via Pydantic.</li> <li>The name of the Unit Task (as registered in the system) and the serialized JSON are sent to the Gateway service.</li> <li>The Gateway service sends the unit task instance to the Resource Manager, which ensures that the Task Manager for the Unit Task is running on the data plane.</li> <li>When the Task Manager is running, the Resource Manager notifies the Task Dispatcher with the unit task instance and Task Manager deployment information.</li> </ol> <p>Deployed Unit Tasks become invocable, either as part of Composite Tasks or directly. Invocation can be driven by a static App driver registered in the Gateway service, a human user via our Jupyter Notebook interface.</p>"},{"location":"architecture/task/#invocation","title":"Invocation","text":"<p>Task invocations go to the Task Dispatcher by calling and awaiting on the async <code>__call__</code> method of the Task. This internally calls all <code>invoke</code> methods of Tasks in the DAG, where each unit Task constructs a <code>TaskInvocation</code> object (task, input, and output) to add to a task-specific <code>TaskContext</code> object. The list of <code>TaskInvocation</code> objects are sent to the Taks Dispatcher.</p> <p>The Task Dispatcher is responsible for actually constructing requests, dispatching them to Task Executors, waiting for the results to come back, and then returning task outputs to the App Driver.  </p>"},{"location":"architecture/task/#deregistration","title":"Deregistration","text":"<p>When an App is unregistered and if there are no other active Apps that require the Task, the Resource Manager will kill the Task Manager and free up the resources.</p>"},{"location":"contributor_guide/","title":"Contributor Guide","text":""},{"location":"contributor_guide/#contributor-guide","title":"Contributor Guide","text":"<p>Here, we provide more info for contributors. General principles are here, and child pages discuss specific topics in more detail.</p> <p>We have a few principles for developing Cornserve:</p> <ol> <li>Strict type annotations: We enforce strict type annotation everywhere in the Python codebase, which leads to numerous benefits including better reliability, readability, and editor support. We use <code>pyright</code> for type checking.</li> <li>Automated testing: We don't aim for 100% test coverage, but non-trivial and/or critical features should be tested with <code>pytest</code>.</li> </ol>"},{"location":"contributor_guide/#contributing-process","title":"Contributing process","text":"<p>Important</p> <p>By contributing to Cornserve, you agree that your code will be licensed with Apache 2.0.</p> <p>If the feature is not small or requires broad changes over the codebase, please open an issue at our GitHub repository to discuss with us.</p> <ol> <li>Fork our GitHub repository. Make sure you clone with <code>--recurse-submodules</code> to get the submodules.</li> <li>Create a new virtual environment:     <pre><code>uv venv --python=3.12\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n</code></pre></li> <li>Install Cornserve in editable mode:     <pre><code># For environments with GPU and nvcc (to compile FlashAttention)\nuv pip install -e 'python[dev]' --config-settings editable_mode=strict\n\n# For environments without GPU (e.g., editor/IDE environment)\nuv pip install -e 'python[dev-no-gpu]' --config-settings editable_mode=strict\n</code></pre></li> <li>Generate Python bindings for Protobuf files with <code>uv run bash scripts/generate_pb.sh</code>.</li> <li>Implement changes in your branch and add tests as needed.</li> <li>Ensure <code>uv run bash python/scripts/lint.sh</code> and <code>pytest</code> passes. Note that many of our tests require GPU.</li> <li>Submit a PR to the main repository. Please ensure that CI (GitHub Actions) passes.</li> </ol>"},{"location":"contributor_guide/#developing-on-kubernetes","title":"Developing on Kubernetes","text":"<p>Cornserve runs on top of Kubernetes, which introduces some complexity in development. Please refer to the guide on Local and Distributed Development on Kubernetes for more details.</p>"},{"location":"contributor_guide/#documentation","title":"Documentation","text":"<p>The documentation is written in Markdown and is located in the <code>docs</code> folder. We use MkDocs to build the documentation and use the <code>mkdocs-material</code> theme.</p> <p>To install documentation build dependencies:</p> <pre><code>uv pip install -r docs/requirements.txt\n</code></pre> <p>To build and preview the documentation:</p> <pre><code>uv run bash scripts/preview_docs.sh\n</code></pre>"},{"location":"contributor_guide/eric/","title":"Eric","text":""},{"location":"contributor_guide/eric/#eric-developer-guide","title":"Eric developer guide","text":""},{"location":"contributor_guide/eric/#docker-container","title":"Docker container","text":"<p>All code is to be run inside a Docker container, including tests.</p> <pre><code>docker build -t cornserve/eric:latest -f docker/eric.Dockerfile .\ndocker run -it --gpus all --entrypoint bash --ipc host --rm --name eric-dev -v $PWD:/workspace/cornserve -v $HF_CACHE:/root/.cache/huggingface cornserve/eric:latest\n</code></pre>"},{"location":"contributor_guide/eric/#editable-installation","title":"Editable installation","text":"<pre><code>pip install -e 'python[dev]'\n</code></pre>"},{"location":"contributor_guide/eric/#testing","title":"Testing","text":"<p>We use <code>pytest</code>. Tests use GPUs.</p> <pre><code>pytest\n</code></pre> <p>Set the <code>CORNSERVE_TEST_DUMP_TENSOR_DIR</code> to an existing directory when running <code>pytest</code>. This will dump output embedding tensors to the specified directory. Refer to <code>build_batch</code> in <code>tests/task_executors/eric/utils.py</code>.</p> <pre><code>export CORNERSERVE_TEST_DUMP_TENSOR_DIR=/path/to/dump\npytest python/tests/task_executors/eric/models/test_llava_onevision.py::test_image_inference\n</code></pre>"},{"location":"contributor_guide/kubernetes/","title":"Developing on Kubernetes","text":""},{"location":"contributor_guide/kubernetes/#local-and-distributed-development-on-kubernetes","title":"Local and Distributed Development on Kubernetes","text":""},{"location":"contributor_guide/kubernetes/#build-and-export-modes","title":"Build and Export Modes","text":"<p>The first step to running Cornserve on Kubernetes is to build container images.</p> <p>We have <code>scripts/build_export_images.sh</code> to build and export container images for Cornserve components. Now, where to push these images changes based on your development environment, the <code>REGISTRY</code> environment variable needs to be set accordingly.</p> Dev Environment <code>REGISTRY</code> Value Description Use Case Local K3s <code>local</code> Builds images directly in K3s containerd (no pull needed) Single-node K3s development Minikube <code>minikube</code> Builds with Docker and loads into Minikube Minikube development Build Only <code>none</code> Builds images locally without pushing anywhere Testing builds Registry Push Registry URL (e.g., <code>myregistry.com:5000</code>) Builds and pushes to specified registry Multi-node clusters or distributed development"},{"location":"contributor_guide/kubernetes/#local-development","title":"Local development","text":"<p>You are developing on a single node. In this case, we don't need a registry. Instead, we build containers directly within the containerd runtime of K3s.</p> <p>To set up your local K3s development environment, first, follow this guide (Section \"Switching from Docker to Containerd\") to set up Nerdctl and BuildKit on your local development machine. It should be something like:</p> <pre><code>wget https://github.com/containerd/nerdctl/releases/download/v2.1.3/nerdctl-2.1.3-linux-amd64.tar.gz\ntar -xvf nerdctl-*.gz\nsudo mv nerdctl /usr/bin\nsudo mkdir -p /etc/nerdctl/\nsudo tee /etc/nerdctl/nerdctl.toml &gt; /dev/null &lt;&lt;'EOF'\naddress        = \"/run/k3s/containerd/containerd.sock\"\nnamespace      = \"k8s.io\"\nEOF\n\nwget https://github.com/moby/buildkit/releases/download/v0.24.0/buildkit-v0.24.0.linux-amd64.tar.gz\ntar -xvf buildkit-v0.24.0.linux-amd64.tar.gz\nsudo mv bin/* /usr/bin\nsudo mkdir -p /etc/buildkit/\nsudo tee /etc/buildkit/buildkitd.toml &gt; /dev/null &lt;&lt;'EOF'\n[worker.oci]\n  enabled = false\n\n[worker.containerd]\n  enabled = true\n  address = \"/run/k3s/containerd/containerd.sock\"\n  namespace = \"k8s.io\"\nEOF\n\nsudo tee /etc/systemd/system/buildkit.service &gt; /dev/null &lt;&lt;'EOF'\n[Unit]\nDescription=BuildKit\nDocumentation=https://github.com/moby/buildkit\n\n[Service]\nType=simple\nTimeoutStartSec=10\nRestart=always\nRestartSec=10\nExecStart=/usr/bin/buildkitd\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\nsudo systemctl daemon-reload\nsudo systemctl enable buildkit.service\nsudo systemctl start buildkit.service\n</code></pre> <p>After that, you can use Nerdctl to build images directly within K3s containerd, and no pull is necessary whatsoever. Use the <code>build_export_images.sh</code> script with the <code>REGISTRY</code> environment variable set to <code>local</code> (a special case):</p> <pre><code>REGISTRY=local bash scripts/build_export_images.sh \n</code></pre> <p>Use the <code>local</code> overlay to deploy Cornserve:</p> <pre><code>kubectl apply -k kustomize/cornserve-system/overlays/local\nkubectl apply -k kustomize/cornserve/overlays/local\n</code></pre> <p>The <code>local</code> overlay specifies <code>imagePullPolicy: Never</code>, meaning that if the image was not found locally, it means that it was not built yet, correctly raising an error.</p> <p>Note</p> <p>You can use the <code>local</code> overlay for the quick Minikube demo as well.</p>"},{"location":"contributor_guide/kubernetes/#distributed-development","title":"Distributed development","text":"<p>You are developing on a multi-node cluster.</p> <p>(1) Now, you do need a registry to push images to, so that remote nodes can pull them:</p> <pre><code>bash kubernetes/registry.sh\nREGISTRY=myregistry.com:5000 bash kubernetes/set_registry.sh  # (1)!\n</code></pre> <ol> <li>Modifies <code>kustomization.yaml</code> and <code>k3s/registries.yaml</code>    If you're on this dev workflow with a single node cluster, you can skip <code>kubernetes/set_registry.sh</code> because things default to <code>localhost:5000</code>.</li> </ol> <p>(2) For K3s to work with insecure (i.e., HTTP) registries, you need to set up the <code>registries.yaml</code> file in <code>/etc/rancher/k3s/</code> on all nodes (master and worker) before starting K3s:</p> <pre><code>sudo cp kubernetes/k3s/registries.yaml /etc/rancher/k3s/registries.yaml\nsudo systemctl start k3s  # or k3s-agent\n</code></pre> <p>(3) On the node that you want to build the images, you also need to specify insecure registries for docker so that it can push images to it. So, in <code>/etc/docker/daemon.json</code>, you should add something like,</p> <pre><code>{\n    \"existing configs...\",\n    \"insecure-registries\": [\"myregistry.com:5000\"]\n}\n</code></pre> <p>Then restart docker by <code>sudo systemctl restart docker</code>.</p> <p>(4) Build and push images to the registry using the <code>build_export_images.sh</code> script with the <code>REGISTRY</code> environment variable set to the registry address:</p> <pre><code>REGISTRY=myregistry.com:5000 bash scripts/build_export_images.sh\n</code></pre> <p>Note</p> <p>Building Eric can consume a lot of memory and may trigger OOMs that freeze the instance. Please set a proper <code>max_jobs</code> in <code>eric.Dockerfile</code>.</p> <p>(5) Use the <code>dev</code> overlay (which specifies <code>imagePullPolicy: Always</code>) to deploy Cornserve:</p> <pre><code>kubectl apply -k kustomize/cornserve-system/overlays/dev\nkubectl apply -k kustomize/cornserve/overlays/dev\n</code></pre>"},{"location":"contributor_guide/sidecar/","title":"Sidecar","text":""},{"location":"contributor_guide/sidecar/#sidecar-developer-guide","title":"Sidecar developer guide","text":""},{"location":"contributor_guide/sidecar/#docker-container","title":"Docker container","text":"<p>It is recommended to run everything inside docker. Sidecar uses <code>UCX</code> as backend, so you might find the <code>docker/dev.Dockerfile</code> helpful. Additionally, Sidecar has  dependency over <code>ucxx-cu12</code>, meaning you need to development on an Nvidia GPU-enabled machine at the moment.</p> <p>Specifying <code>--shm-size</code> with at least 4 GB and <code>--ipc host</code> is required.</p>"},{"location":"contributor_guide/sidecar/#editable-installation","title":"Editable installation","text":"<pre><code>pip install -e 'python[dev]'\n</code></pre>"},{"location":"contributor_guide/sidecar/#testing","title":"Testing","text":"<p>We use pytest.</p> <pre><code>pytest python/tests/services/sidecar/test_sidecar.py\n</code></pre> <p>When testing locally with task executors, you can <code>export SIDECAR_IS_LOCAL=true</code> to route all communications through <code>localhost</code> instead of k8s network.</p>"},{"location":"contributor_guide/sidecar/#testing-with-mock-sidecar","title":"Testing with Mock Sidecar","text":"<p>You can test components (e.g., Geri, Task Executors) that use sidecar clients without spawning the sidecar server, running k3s, or doing any communication by using the mock sidecar mode. This is particularly useful for local development and testing.</p>"},{"location":"contributor_guide/sidecar/#setup","title":"Setup","text":"<ol> <li> <p>Enable mock sidecar mode inside your Docker container:    </p><pre><code>export CORNSERVE_MOCK_SIDECAR=1\n</code></pre> </li> <li> <p>Generate and export a mock mapping. The mapping is a JSON object that maps <code>{data_id}-{chunk_id}</code> to file paths where the sidecar will read/write data:    </p><pre><code>import json\n\ntensor_data_id = \"audio_code\"\njson_data_id = \"metadata\"\n\n# Generate mapping\n# Streaming audio code (two chunks), single-chunk JSON metadata\nmock_mapping = {\n    f\"{tensor_data_id}-0\": \"/path/to/code0.pt\",\n    f\"{tensor_data_id}-1\": \"/path/to/code1.pt\",\n    f\"{json_data_id}-0\": \"/path/to/data.json\",\n    # Add more mappings as needed\n}\n\n# Export as environment variable\nos.environ[\"CORNSERVE_MOCK_SIDECAR_MAPPING\"] = json.dumps(mock_mapping)\n</code></pre> </li> <li> <p>Fire sends and receives with the sidecar client using the same data and chunk IDs as in the mapping. The sidecar client will read from/write to the specified file paths instead of actual network communication.</p> </li> </ol>"},{"location":"contributor_guide/sidecar/#example","title":"Example","text":"<p>See <code>python/tests/services/sidecar/test_mock_sidecar.py</code> for a complete example. The test demonstrates creating a mock mapping with various data types (tensors, chunked tensors, <code>dict</code>, primitives like <code>int</code>).</p>"},{"location":"contributor_guide/sidecar/#debugging","title":"Debugging","text":"<p>To debug UCX related error, you can set <code>UCX_LOG_LEVEL=trace</code> and <code>UCXPY_LOG_LEVEL=DEBUG</code></p>"},{"location":"contributor_guide/tracing/","title":"Tracing","text":""},{"location":"contributor_guide/tracing/#tracing-developer-guide","title":"Tracing Developer guide","text":"<p>We employ OpenTelemetry for observability. Below are some of the conventions we use.</p> <p>Generally, we use auto-instrumentation provided by OpenTelemetry, e.g., FastAPI, gRPC, HTTPX.</p>"},{"location":"contributor_guide/tracing/#spans","title":"Spans","text":"<p>Usually named with <code>ClassName.function_name</code>.</p>"},{"location":"contributor_guide/tracing/#attributes","title":"Attributes","text":"<p>Usually named with <code>namespace.subroutine.attribute_name</code>. <code>namespace</code> is typically the name of the service, like <code>gateway</code>.</p>"},{"location":"contributor_guide/tracing/#events","title":"Events","text":"<p>Usually named with <code>action.event_name</code>. Use spans for things that happen over time (e.g., a subroutine), where tracking the start and end is important. On the other hand, use events for singular occurrences that happen at a specific moment in time.</p>"},{"location":"contributor_guide/tracing/#test","title":"Test","text":"<p>When testing locally, you can disable OTEL tracing through <code>OTEL_SDK_DISABLED=true</code>.</p>"},{"location":"getting_started/","title":"Getting Started","text":""},{"location":"getting_started/#getting-started","title":"Getting Started","text":""},{"location":"getting_started/#try-it-out-in-minikube","title":"Try it out in Minikube!","text":"<p>You can try out Cornserve on your local machine (with Docker and at least two NVIDIA GPUs(1)) using Minikube.</p> <ol> <li>Compute Capability &gt;= 8.0</li> </ol> <p>First, install Minikube following their guide.</p> <p>Then, start a Minikube cluster with GPU support (1):</p> <ol> <li>We recommend enabling rootless docker to avoid permission or <code>$PATH</code> related issues.</li> </ol> <pre><code>minikube start \\\n    --driver docker \\\n    --container-runtime docker \\\n    --gpus all \\\n    --disk-size 50g  # (1)!\n</code></pre> <ol> <li>Give it enough disk space to download model weights and stuff. You can also give more CPU (e.g., <code>--cpus 8</code>) and memory (<code>--memory 16g</code>).</li> </ol> <p>Next, and this is important, we want to increase the shared memory (<code>/dev/shm</code>) size of the Minikube container.</p> <pre><code>minikube ssh -- sudo mount -o remount,size=16G /dev/shm\n</code></pre> <p>Next, clone the Cornserve GitHub repository and deploy Cornserve on your Minikube cluster:</p> <pre><code>git clone https://github.com/cornserve-ai/cornserve.git\ncd cornserve\ngit checkout v0.0.2  # or the latest release tag\n\nminikube kubectl -- apply -k kubernetes/kustomize/cornserve-system/overlays/minikube\nminikube kubectl -- apply -k kubernetes/kustomize/cornserve/overlays/minikube\n</code></pre> <p>We'll be using Gemma 3 4B for this demo, so you need to have access (requests are processed immediately with an account). While we wait for the containers to spin up, add your HuggingFace access token to Cornserve, which can be created here if you don't have one already. </p><pre><code>minikube kubectl -- create -n cornserve secret generic cornserve-env --from-literal=hf-token='YOUR_HUGGINGFACE_TOKEN'\n</code></pre> <p>After a few moments (which largely depends on how long it takes to pull Docker images from Docker Hub), check whether Cornserve is running:</p> <pre><code>$ minikube kubectl -- get -n cornserve pods   # (1)!\nNAME                               READY   STATUS    RESTARTS   AGE\ngateway-6c65745c5d-x8gkh           1/1     Running   0          4s\nresource-manager-9b4df4687-9djc4   1/1     Running   0          4s\ntask-dispatcher-9954cffcd-g4rk2    1/1     Running   0          4s\nsidecar-0                          1/1     Running   0          3s\nsidecar-1                          1/1     Running   0          3s\nsidecar-2                          1/1     Running   0          3s\nsidecar-3                          1/1     Running   0          3s\n</code></pre> <ol> <li>The number of Sidecar pods should match the number of GPUs you gave to Minikube. They are spawned by the Resource Manager, so you will initially see only three (Gateway, Resource Manager, and Task Dispatcher) pods running.</li> </ol> <p>Next, install the Cornserve CLI that helps you interact with Cornserve:</p> <pre><code>uv venv --python=3.11\nsource .venv/bin/activate\nuv pip install python/\n</code></pre> <p>Next, we need to deploy built-in tasks that Cornserve provides to our Cornserve cluster.</p> <pre><code>uv pip install python-tasklib/\nexport CORNSERVE_GATEWAY_URL=$(minikube service -n cornserve gateway-node-port --url)\ncornserve deploy-tasklib\n</code></pre> <p>Then, try registering a simple example app that defines a Vision-Language Model:</p> <pre><code>cornserve register examples/mllm.py\n</code></pre> <p>You can check out example apps on GitHub.</p> <p>This will take a few minutes; mainly (1) pulling in the Docker images and (2) waiting for vLLM to warm up and start. But eventually, you should see something like this:</p> <pre><code>$ cornserve register examples/mllm.py\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 App ID                               \u2502 Alias \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 app-564b79ff446342c69821464b22585a72 \u2502 mllm  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>Now, you can invoke the app using the CLI:</p> <pre><code>$ cornserve invoke mllm --aggregate-keys choices.0.delta.content --data - &lt;&lt;EOF\nmodel: \"Qwen/Qwen2-VL-7B-Instruct\"\nmessages:\n- role: \"user\"\n  content:\n  - type: text\n    text: \"Write a poem about the images you see.\"\n  - type: image_url\n    image_url:\n      url: \"https://picsum.photos/id/12/480/560\"\n  - type: image_url\n    image_url:\n      url: \"https://picsum.photos/id/234/960/960\"\nEOF\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 choices.0.delta.content \u2502 Okay, here are haikus for each image:    \u2502\n\u2502                         \u2502                                          \u2502\n\u2502                         \u2502 **Image 1: Coastal Landscape**           \u2502\n\u2502                         \u2502                                          \u2502\n\u2502                         \u2502 Gray sea meets the shore,                \u2502\n\u2502                         \u2502 Rocks stand still, a weathered grace,    \u2502\n\u2502                         \u2502 Island dreams unfold.                    \u2502\n\u2502                         \u2502                                          \u2502\n\u2502                         \u2502 **Image 2: Paris Scene**                 \u2502\n\u2502                         \u2502                                          \u2502\n\u2502                         \u2502 Fog veils city\u2019s height,                 \u2502\n\u2502                         \u2502 Eiffel stands, a ghostly trace,          \u2502\n\u2502                         \u2502 Winter\u2019s quiet grace.                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>You can learn more about defining apps (and tasks) in our guide.</p> <p>Here's how to clean up:</p> <pre><code>minikube kubectl -- delete -k kubernetes/kustomize/cornserve/overlays/minikube\nminikube kubectl -- delete -k kubernetes/kustomize/cornserve-system/overlays/minikube\nminikube stop  # or minikube delete\n</code></pre>"},{"location":"getting_started/#getting-started-seriously","title":"Getting started (seriously)","text":"<p>At a high level, there are two steps to using Cornserve:</p> <ol> <li>Cornserve deployment: Deploying Cornserve on a GPU cluster managed by Kubernetes.</li> <li>Building your app: Building a Cornserve app and deploying it on a Cornserve cluster for invocation.</li> </ol> <ol> <li>Registering and invoking your app: Building a Cornserve app and deploying it on a Cornserve cluster for invocation.</li> </ol>"},{"location":"getting_started/building_apps/","title":"Building Apps","text":""},{"location":"getting_started/building_apps/#building-apps","title":"Building Apps","text":"<p>Cornserve has two layers of defining execution:</p> <ul> <li>App: This is the highest level of construct, which takes a request and returns a response. Apps are written in Python and can be submitted to the Cornserve Gateway for deployment.</li> <li>Task: This is a unit of work that is executed by the Cornserve data plane. There are two types of tasks:<ul> <li>Unit Task: Unit Tasks are the smallest and most basic type of task. They are executed in a single Kubernetes Pod and are the unit of scaling. For instance, there is the built-in modality embedding unit task which embeds specific modalities (e.g., image, video, audio), which is executed by our Eric server. There is also the built-in LLM text generation task, which generates text from input text prompts and any embedded modalities.</li> <li>Composite Task: Composite Tasks are a composition of one or more Unit Tasks. They are defined by the user in Python. For instance, there is the built-in Multimodal LLM composite task which instantiates modality embedding unit tasks as needed, runs them on multimodal data to embed them, and passes them to the LLM text generation unit task to generate text. Intermediate data produced by unit tasks are forwarded directly to the next unit task in the graph.</li> </ul> </li> </ul>"},{"location":"getting_started/building_apps/#example-gemma-arena","title":"Example: Gemma Arena","text":"<p>Let's look at a real example app that demonstrates streaming responses from different Gemma models simultaneously. This <code>gemmarena.py</code> app lets users compare different Gemma models side by side, like an arena.</p> <pre><code>import asyncio\nfrom collections.abc import AsyncIterator\n\nfrom cornserve_tasklib.task.composite.llm import MLLMTask\nfrom cornserve_tasklib.task.unit.encoder import Modality\nfrom cornserve_tasklib.task.unit.llm import OpenAIChatCompletionChunk, OpenAIChatCompletionRequest\nfrom pydantic import RootModel\n\nfrom cornserve.app.base import AppConfig\nfrom cornserve.task.base import Stream\n\n# Comment out any Gemma models you don't want to include in the arena.\ngemma_model_ids = {\n    \"gemma3-4b\": \"google/gemma-3-4b-it\",\n    \"gemma3-12b\": \"google/gemma-3-12b-it\",\n    \"gemma3-27b\": \"google/gemma-3-27b-it\",\n}\n\n\n# All Gemma MLLMTasks will share the same encoder task deployment.\ngemma_tasks = {\n    name: MLLMTask(\n        modalities=[Modality.IMAGE],\n        model_id=model_id,\n        encoder_model_ids=set(gemma_model_ids.values()),\n    )\n    for name, model_id in gemma_model_ids.items()\n}\n\n\nclass ArenaOutput(RootModel[dict[str, str]]):\n    \"\"\"Response model for the app.\n\n    Wrapper around a dictionary that maps model names to generated text chunks.\n    \"\"\"\n\n\nclass Config(AppConfig):\n    \"\"\"App configuration model.\"\"\"\n\n    tasks = {**gemma_tasks}\n\n\nasync def serve(request: OpenAIChatCompletionRequest) -&gt; AsyncIterator[ArenaOutput]:\n    \"\"\"Main serve function for the app.\"\"\"\n    # NOTE: Doing `await` for each task separately will make them run sequentially.\n    tasks: list[asyncio.Task[Stream[OpenAIChatCompletionChunk]]] = []\n    for task in gemma_tasks.values():\n        # Overwrite the model ID in the request to match the task's model ID.\n        request_ = request.model_copy(update={\"model\": task.model_id}, deep=True)\n        tasks.append(asyncio.create_task(task(request_)))\n\n    # An await is needed to actually dispatch the tasks.\n    # Responses will be streamed after this.\n    dispatch_responses = await asyncio.gather(*tasks)\n\n    streams = {\n        asyncio.create_task(anext(stream)): (stream, name)\n        for stream, name in zip(dispatch_responses, gemma_model_ids.keys(), strict=True)\n    }\n\n    while streams:\n        done, _ = await asyncio.wait(streams.keys(), return_when=asyncio.FIRST_COMPLETED)\n\n        for task in done:\n            stream, name = streams.pop(task)\n\n            try:\n                chunk = task.result()\n            except StopAsyncIteration:\n                # This model is done responding.\n                continue\n\n            delta = chunk.choices[0].delta.content\n            if delta is None:\n                continue\n            yield ArenaOutput({name: delta})\n\n            streams[asyncio.create_task(anext(stream))] = (stream, name)\n</code></pre> <p>Let's break this down.</p>"},{"location":"getting_started/building_apps/#key-components","title":"Key Components","text":"<ol> <li> <p>Built-in Tasks: The app uses Cornserve's built-in <code>MLLMTask</code> (Multimodal LLM Task) rather than defining custom composite tasks. This task handles both image encoding and text generation.</p> </li> <li> <p>Task Declaration: Multiple Gemma models are configured with shared encoder deployments:    </p><pre><code>gemma_tasks = {\n    name: MLLMTask(\n        modalities=[Modality.IMAGE],\n        model_id=model_id,\n        encoder_model_ids=set(gemma_model_ids.values()),\n    )\n    for name, model_id in gemma_model_ids.items()\n}\n</code></pre> </li> <li> <p>App Configuration: The <code>Config</code> class registers all the tasks with the Cornserve platform:    </p><pre><code>class Config(AppConfig):\n    tasks = {**gemma_tasks}\n</code></pre> </li> <li> <p>The Async <code>serve</code> function: This is the main entry point of the app, taking a Pydantic model as input (request). It handles incoming requests and orchestrates parallel execution of multiple unit/composite tasks.    </p><pre><code>async def serve(\n    request: OpenAIChatCompletionRequest,\n) -&gt; AsyncIterator[ArenaOutput]:\n    # Invoke all Gemma tasks concurrently\n    # Wait for all three concurrently, and yield chunks as they arrive\n    ...\n</code></pre> </li> <li> <p>App Responses: The app's <code>serve</code> function can either return a single response object (Pydantic model), or instead return an async iterator that yields response chunks (each chunk being a Pydantic model) for streaming responses. The Gemma Arena app demonstrates streaming responses.</p> </li> </ol>"},{"location":"getting_started/building_apps/#benefits","title":"Benefits","text":"<ul> <li>Flexibility: The app's asynchronous <code>serve</code> function allows the full flexibility of Python programming, including concurrency with <code>asyncio</code>. This enables complex orchestration patterns, such as parallel execution and streaming responses.</li> <li>Automatic Sharing: In the app, we defined three Gemma <code>MLLMTask</code>s. In fact, the three Gemma models share the exact same vision encoder, so we can share a single vision encoder for all three Gemma tasks. Cornserve automatically detects this (by checking whether the <code>EncoderTask</code> inside the three <code>MLLMTask</code>s are identical) and only deploys a single vision encoder for all three tasks.</li> </ul>"},{"location":"getting_started/building_apps/#next-steps","title":"Next Steps","text":"<p>Now that you've learned how to build apps with Cornserve, the next step is to deploy and run your app. Head over to Deploying Apps to Cornserve to learn how to register your <code>gemmarena.py</code> app and invoke it with real requests.</p>"},{"location":"getting_started/cornserve/","title":"Deploying Cornserve","text":""},{"location":"getting_started/cornserve/#deploying-cornserve","title":"Deploying Cornserve","text":"<p>Cornserve can be deployed on a GPU cluster managed by Kubernetes.</p> <p>Note</p> <p>The <code>cornserve</code> namespace is used for most of our control plane and data plane objects. On the other hand, the <code>cornserve-system</code> namespace is used for components that look over and manage the Cornserve system itself (under <code>cornserve</code>), like Jaeger and Prometheus.</p> <p>If you already have a Kubernetes cluster running, you can deploy Cornserve on it with the <code>prod</code> overlay:</p>"},{"location":"getting_started/cornserve/#deploying-k3s","title":"Deploying K3s","text":"<p>Tip</p> <p>If you have a Kubernetes cluster running, you can skip this section.</p> <p>If you don't have a Kubernetes cluster running, you can deploy Cornserve on a K3s cluster. We also use the K3s distribution of Kubernetes for our development. Refer to their Documentation for more details.</p> <p>Tip</p> <p>If you're deploying on-premise with k3s, make sure you have plenty of disk space under <code>/var/lib/rancher</code> because <code>containerd</code> stores images there. If not, you can create a directory in a secondary storage (e.g., <code>/mnt/data/rancher</code>) and symlink it to <code>/var/lib/rancher</code> prior to starting k3s.</p>"},{"location":"getting_started/cornserve/#clone-the-repository","title":"Clone the Repository","text":"<pre><code>git clone https://github.com/cornserve-ai/cornserve.git\ncd cornserve/kubernetes\n</code></pre>"},{"location":"getting_started/cornserve/#master-node","title":"Master Node","text":"<p>Install and start K3s:</p> <pre><code>curl -sfL https://get.k3s.io | INSTALL_K3S_SKIP_ENABLE=true sh -\nsudo mkdir -p /etc/rancher/k3s\nsudo cp k3s/server-config.yaml /etc/rancher/k3s/config.yaml\nsudo systemctl start k3s\n</code></pre> <p>Note the master node address (<code>$MASTER_ADDRESS</code>) and the node token (<code>$NODE_TOKEN</code>):</p> <pre><code>NODE_TOKEN=\"$(sudo cat /var/lib/rancher/k3s/server/node-token)\"\n</code></pre>"},{"location":"getting_started/cornserve/#worker-nodes","title":"Worker Nodes","text":"<p>Install and start K3s:</p> <pre><code>curl -sfL https://get.k3s.io | K3S_URL=https://$MASTER_ADDRESS:6443 K3S_TOKEN=$NODE_TOKEN INSTALL_K3S_SKIP_ENABLE=true sh -\nsudo mkdir -p /etc/rancher/k3s\nsudo cp k3s/agent-config.yaml /etc/rancher/k3s/config.yaml\nsudo systemctl start k3s-agent\n</code></pre>"},{"location":"getting_started/cornserve/#nvidia-device-plugin","title":"NVIDIA Device Plugin","text":"<p>The NVIDIA GPU Device Plugin is required to expose GPUs to the Kubernetes cluster as resources. You can deploy a specific version like this:</p> <pre><code>kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.17.3/deployments/static/nvidia-device-plugin.yml\n</code></pre>"},{"location":"getting_started/cornserve/#deploying-cornserve_1","title":"Deploying Cornserve","text":"<p>If you haven't already, clone the Cornserve repository:</p> <pre><code>git clone git@github.com:cornserve-ai/cornserve.git\ncd cornserve\n</code></pre> <p>On top of a Kubernetes cluster, you can deploy Cornserve with a single command:</p> <pre><code>kubectl apply -k kubernetes/kustomize/cornserve-system/base\nkubectl apply -k kubernetes/kustomize/cornserve/overlays/prod\n</code></pre> <p>If you'll be using gated models from Hugging Face Hub, you'll need to make the Hugging Face token available to Task Executors:</p> <pre><code>kubectl create -n cornserve secret generic cornserve-env --from-literal=hf-token=$HF_TOKEN\n</code></pre> <p>Note</p> <p>The <code>cornserve</code> namespace is used for most of our control plane and data plane objects. On the other hand, the <code>cornserve-system</code> namespace is used for components that look over and manage the Cornserve system itself (under <code>cornserve</code>), like Jaeger and Prometheus.</p>"},{"location":"getting_started/registering_apps/","title":"Registering and Invoking Apps","text":""},{"location":"getting_started/registering_apps/#deploying-apps-to-cornserve-and-invoking-them","title":"Deploying Apps to Cornserve and Invoking Them","text":"<p>Once you've written your app, you can deploy it to Cornserve. The current deployment process is as follows:</p> <ol> <li>Save the app code in a single Python file (e.g., <code>gemmarena.py</code>).</li> <li>If you haven't already, deploy the Cornserve tasklib to the cluster.     <pre><code>export CORNSERVE_GATEWAY_URL=[...]\ncornserve deploy-tasklib\n</code></pre></li> <li>Register &amp; deploy the app to the Cornserve Gateway for validation and deployment:     <pre><code>cornserve register gemmarena.py\n</code></pre></li> <li>When validation succeeds, the Cornserve Gateway will deploy the app and all its subtasks on the Cornserve data plane, and the <code>cornserve</code> CLI invocation will return with the app's ID.</li> <li>Finally, you can invoke the app using the Cornserve CLI or send requests to the Cornserve Gateway with your choice of HTTP client.</li> </ol>"},{"location":"getting_started/registering_apps/#using-the-cornserve-cli","title":"Using the Cornserve CLI","text":"<p>For streaming responses like our Gemma Arena example, you can use the CLI to invoke the app:</p> <pre><code>cornserve invoke gemmarena --aggregate-keys gemma3-4b gemma3-12b gemma3-27b --data - &lt;&lt;EOF\nmodel: gemmas\nmessages:\n- role: \"user\"\n  content:\n  - type: text\n    text: \"Write a poem about the images you see.\"\n  - type: image_url\n    image_url:\n      url: \"https://picsum.photos/id/12/480/560\"\n  - type: image_url\n    image_url:\n      url: \"https://picsum.photos/id/234/960/960\"\nEOF\n</code></pre> <p>Notice that this is basically a YAML representation of <code>OpenAIChatCompletionRequest</code>.</p>"},{"location":"getting_started/registering_apps/#next-steps","title":"Next Steps","text":"<p>To dive deeper into the architecture of Cornserve, check out our architecture guide.</p>"}]}